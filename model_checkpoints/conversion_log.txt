roberta.embeddings.word_embeddings.weight ==> roberta.embeddings.word_embeddings.weight (torch.Size([250002, 768]), False)
roberta.embeddings.position_embeddings.weight ==> roberta.embeddings.position_embeddings.weight (torch.Size([514, 768]), False)
roberta.embeddings.token_type_embeddings.weight ==> roberta.embeddings.token_type_embeddings.weight (torch.Size([1, 768]), False)
roberta.embeddings.LayerNorm.weight ==> roberta.embeddings.LayerNorm.weight (torch.Size([768]), False)
roberta.embeddings.LayerNorm.bias ==> roberta.embeddings.LayerNorm.bias (torch.Size([768]), False)
roberta.encoder.layer.0.attention.self.query.weight ==> roberta.encoder.layers.0.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.0.attention.self.query.bias ==> roberta.encoder.layers.0.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.0.attention.self.key.weight ==> roberta.encoder.layers.0.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.0.attention.self.key.bias ==> roberta.encoder.layers.0.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.0.attention.self.value.weight ==> roberta.encoder.layers.0.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.0.attention.self.value.bias ==> roberta.encoder.layers.0.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.0.attention.output.dense.weight ==> roberta.encoder.layers.0.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.0.attention.output.dense.bias ==> roberta.encoder.layers.0.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.0.attention.output.LayerNorm.weight ==> roberta.encoder.layers.0.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.0.attention.output.LayerNorm.bias ==> roberta.encoder.layers.0.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.0.intermediate.dense.weight ==> roberta.encoder.layers.0.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.0.intermediate.dense.bias ==> roberta.encoder.layers.0.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.0.output.dense.weight ==> roberta.encoder.layers.0.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.0.output.dense.bias ==> roberta.encoder.layers.0.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.0.output.LayerNorm.weight ==> roberta.encoder.layers.0.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.0.output.LayerNorm.bias ==> roberta.encoder.layers.0.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.1.attention.self.query.weight ==> roberta.encoder.layers.1.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.1.attention.self.query.bias ==> roberta.encoder.layers.1.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.1.attention.self.key.weight ==> roberta.encoder.layers.1.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.1.attention.self.key.bias ==> roberta.encoder.layers.1.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.1.attention.self.value.weight ==> roberta.encoder.layers.1.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.1.attention.self.value.bias ==> roberta.encoder.layers.1.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.1.attention.output.dense.weight ==> roberta.encoder.layers.1.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.1.attention.output.dense.bias ==> roberta.encoder.layers.1.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.1.attention.output.LayerNorm.weight ==> roberta.encoder.layers.1.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.1.attention.output.LayerNorm.bias ==> roberta.encoder.layers.1.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.1.intermediate.dense.weight ==> roberta.encoder.layers.1.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.1.intermediate.dense.bias ==> roberta.encoder.layers.1.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.1.output.dense.weight ==> roberta.encoder.layers.1.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.1.output.dense.bias ==> roberta.encoder.layers.1.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.1.output.LayerNorm.weight ==> roberta.encoder.layers.1.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.1.output.LayerNorm.bias ==> roberta.encoder.layers.1.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.2.attention.self.query.weight ==> roberta.encoder.layers.2.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.2.attention.self.query.bias ==> roberta.encoder.layers.2.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.2.attention.self.key.weight ==> roberta.encoder.layers.2.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.2.attention.self.key.bias ==> roberta.encoder.layers.2.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.2.attention.self.value.weight ==> roberta.encoder.layers.2.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.2.attention.self.value.bias ==> roberta.encoder.layers.2.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.2.attention.output.dense.weight ==> roberta.encoder.layers.2.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.2.attention.output.dense.bias ==> roberta.encoder.layers.2.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.2.attention.output.LayerNorm.weight ==> roberta.encoder.layers.2.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.2.attention.output.LayerNorm.bias ==> roberta.encoder.layers.2.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.2.intermediate.dense.weight ==> roberta.encoder.layers.2.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.2.intermediate.dense.bias ==> roberta.encoder.layers.2.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.2.output.dense.weight ==> roberta.encoder.layers.2.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.2.output.dense.bias ==> roberta.encoder.layers.2.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.2.output.LayerNorm.weight ==> roberta.encoder.layers.2.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.2.output.LayerNorm.bias ==> roberta.encoder.layers.2.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.3.attention.self.query.weight ==> roberta.encoder.layers.3.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.3.attention.self.query.bias ==> roberta.encoder.layers.3.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.3.attention.self.key.weight ==> roberta.encoder.layers.3.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.3.attention.self.key.bias ==> roberta.encoder.layers.3.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.3.attention.self.value.weight ==> roberta.encoder.layers.3.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.3.attention.self.value.bias ==> roberta.encoder.layers.3.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.3.attention.output.dense.weight ==> roberta.encoder.layers.3.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.3.attention.output.dense.bias ==> roberta.encoder.layers.3.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.3.attention.output.LayerNorm.weight ==> roberta.encoder.layers.3.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.3.attention.output.LayerNorm.bias ==> roberta.encoder.layers.3.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.3.intermediate.dense.weight ==> roberta.encoder.layers.3.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.3.intermediate.dense.bias ==> roberta.encoder.layers.3.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.3.output.dense.weight ==> roberta.encoder.layers.3.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.3.output.dense.bias ==> roberta.encoder.layers.3.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.3.output.LayerNorm.weight ==> roberta.encoder.layers.3.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.3.output.LayerNorm.bias ==> roberta.encoder.layers.3.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.4.attention.self.query.weight ==> roberta.encoder.layers.4.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.4.attention.self.query.bias ==> roberta.encoder.layers.4.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.4.attention.self.key.weight ==> roberta.encoder.layers.4.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.4.attention.self.key.bias ==> roberta.encoder.layers.4.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.4.attention.self.value.weight ==> roberta.encoder.layers.4.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.4.attention.self.value.bias ==> roberta.encoder.layers.4.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.4.attention.output.dense.weight ==> roberta.encoder.layers.4.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.4.attention.output.dense.bias ==> roberta.encoder.layers.4.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.4.attention.output.LayerNorm.weight ==> roberta.encoder.layers.4.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.4.attention.output.LayerNorm.bias ==> roberta.encoder.layers.4.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.4.intermediate.dense.weight ==> roberta.encoder.layers.4.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.4.intermediate.dense.bias ==> roberta.encoder.layers.4.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.4.output.dense.weight ==> roberta.encoder.layers.4.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.4.output.dense.bias ==> roberta.encoder.layers.4.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.4.output.LayerNorm.weight ==> roberta.encoder.layers.4.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.4.output.LayerNorm.bias ==> roberta.encoder.layers.4.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.5.attention.self.query.weight ==> roberta.encoder.layers.5.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.5.attention.self.query.bias ==> roberta.encoder.layers.5.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.5.attention.self.key.weight ==> roberta.encoder.layers.5.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.5.attention.self.key.bias ==> roberta.encoder.layers.5.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.5.attention.self.value.weight ==> roberta.encoder.layers.5.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.5.attention.self.value.bias ==> roberta.encoder.layers.5.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.5.attention.output.dense.weight ==> roberta.encoder.layers.5.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.5.attention.output.dense.bias ==> roberta.encoder.layers.5.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.5.attention.output.LayerNorm.weight ==> roberta.encoder.layers.5.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.5.attention.output.LayerNorm.bias ==> roberta.encoder.layers.5.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.5.intermediate.dense.weight ==> roberta.encoder.layers.5.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.5.intermediate.dense.bias ==> roberta.encoder.layers.5.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.5.output.dense.weight ==> roberta.encoder.layers.5.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.5.output.dense.bias ==> roberta.encoder.layers.5.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.5.output.LayerNorm.weight ==> roberta.encoder.layers.5.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.5.output.LayerNorm.bias ==> roberta.encoder.layers.5.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.6.attention.self.query.weight ==> roberta.encoder.layers.6.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.6.attention.self.query.bias ==> roberta.encoder.layers.6.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.6.attention.self.key.weight ==> roberta.encoder.layers.6.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.6.attention.self.key.bias ==> roberta.encoder.layers.6.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.6.attention.self.value.weight ==> roberta.encoder.layers.6.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.6.attention.self.value.bias ==> roberta.encoder.layers.6.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.6.attention.output.dense.weight ==> roberta.encoder.layers.6.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.6.attention.output.dense.bias ==> roberta.encoder.layers.6.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.6.attention.output.LayerNorm.weight ==> roberta.encoder.layers.6.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.6.attention.output.LayerNorm.bias ==> roberta.encoder.layers.6.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.6.intermediate.dense.weight ==> roberta.encoder.layers.6.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.6.intermediate.dense.bias ==> roberta.encoder.layers.6.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.6.output.dense.weight ==> roberta.encoder.layers.6.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.6.output.dense.bias ==> roberta.encoder.layers.6.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.6.output.LayerNorm.weight ==> roberta.encoder.layers.6.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.6.output.LayerNorm.bias ==> roberta.encoder.layers.6.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.7.attention.self.query.weight ==> roberta.encoder.layers.7.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.7.attention.self.query.bias ==> roberta.encoder.layers.7.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.7.attention.self.key.weight ==> roberta.encoder.layers.7.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.7.attention.self.key.bias ==> roberta.encoder.layers.7.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.7.attention.self.value.weight ==> roberta.encoder.layers.7.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.7.attention.self.value.bias ==> roberta.encoder.layers.7.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.7.attention.output.dense.weight ==> roberta.encoder.layers.7.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.7.attention.output.dense.bias ==> roberta.encoder.layers.7.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.7.attention.output.LayerNorm.weight ==> roberta.encoder.layers.7.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.7.attention.output.LayerNorm.bias ==> roberta.encoder.layers.7.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.7.intermediate.dense.weight ==> roberta.encoder.layers.7.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.7.intermediate.dense.bias ==> roberta.encoder.layers.7.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.7.output.dense.weight ==> roberta.encoder.layers.7.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.7.output.dense.bias ==> roberta.encoder.layers.7.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.7.output.LayerNorm.weight ==> roberta.encoder.layers.7.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.7.output.LayerNorm.bias ==> roberta.encoder.layers.7.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.8.attention.self.query.weight ==> roberta.encoder.layers.8.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.8.attention.self.query.bias ==> roberta.encoder.layers.8.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.8.attention.self.key.weight ==> roberta.encoder.layers.8.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.8.attention.self.key.bias ==> roberta.encoder.layers.8.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.8.attention.self.value.weight ==> roberta.encoder.layers.8.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.8.attention.self.value.bias ==> roberta.encoder.layers.8.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.8.attention.output.dense.weight ==> roberta.encoder.layers.8.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.8.attention.output.dense.bias ==> roberta.encoder.layers.8.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.8.attention.output.LayerNorm.weight ==> roberta.encoder.layers.8.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.8.attention.output.LayerNorm.bias ==> roberta.encoder.layers.8.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.8.intermediate.dense.weight ==> roberta.encoder.layers.8.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.8.intermediate.dense.bias ==> roberta.encoder.layers.8.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.8.output.dense.weight ==> roberta.encoder.layers.8.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.8.output.dense.bias ==> roberta.encoder.layers.8.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.8.output.LayerNorm.weight ==> roberta.encoder.layers.8.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.8.output.LayerNorm.bias ==> roberta.encoder.layers.8.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.9.attention.self.query.weight ==> roberta.encoder.layers.9.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.9.attention.self.query.bias ==> roberta.encoder.layers.9.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.9.attention.self.key.weight ==> roberta.encoder.layers.9.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.9.attention.self.key.bias ==> roberta.encoder.layers.9.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.9.attention.self.value.weight ==> roberta.encoder.layers.9.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.9.attention.self.value.bias ==> roberta.encoder.layers.9.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.9.attention.output.dense.weight ==> roberta.encoder.layers.9.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.9.attention.output.dense.bias ==> roberta.encoder.layers.9.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.9.attention.output.LayerNorm.weight ==> roberta.encoder.layers.9.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.9.attention.output.LayerNorm.bias ==> roberta.encoder.layers.9.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.9.intermediate.dense.weight ==> roberta.encoder.layers.9.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.9.intermediate.dense.bias ==> roberta.encoder.layers.9.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.9.output.dense.weight ==> roberta.encoder.layers.9.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.9.output.dense.bias ==> roberta.encoder.layers.9.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.9.output.LayerNorm.weight ==> roberta.encoder.layers.9.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.9.output.LayerNorm.bias ==> roberta.encoder.layers.9.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.10.attention.self.query.weight ==> roberta.encoder.layers.10.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.10.attention.self.query.bias ==> roberta.encoder.layers.10.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.10.attention.self.key.weight ==> roberta.encoder.layers.10.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.10.attention.self.key.bias ==> roberta.encoder.layers.10.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.10.attention.self.value.weight ==> roberta.encoder.layers.10.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.10.attention.self.value.bias ==> roberta.encoder.layers.10.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.10.attention.output.dense.weight ==> roberta.encoder.layers.10.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.10.attention.output.dense.bias ==> roberta.encoder.layers.10.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.10.attention.output.LayerNorm.weight ==> roberta.encoder.layers.10.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.10.attention.output.LayerNorm.bias ==> roberta.encoder.layers.10.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.10.intermediate.dense.weight ==> roberta.encoder.layers.10.linear1.weight (torch.Size([768, 3072]), True)
roberta.encoder.layer.10.intermediate.dense.bias ==> roberta.encoder.layers.10.linear1.bias (torch.Size([3072]), False)
roberta.encoder.layer.10.output.dense.weight ==> roberta.encoder.layers.10.linear2.weight (torch.Size([3072, 768]), True)
roberta.encoder.layer.10.output.dense.bias ==> roberta.encoder.layers.10.linear2.bias (torch.Size([768]), False)
roberta.encoder.layer.10.output.LayerNorm.weight ==> roberta.encoder.layers.10.norm2.weight (torch.Size([768]), False)
roberta.encoder.layer.10.output.LayerNorm.bias ==> roberta.encoder.layers.10.norm2.bias (torch.Size([768]), False)
roberta.encoder.layer.11.attention.self.query.weight ==> roberta.encoder.layers.11.self_attn.q_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.11.attention.self.query.bias ==> roberta.encoder.layers.11.self_attn.q_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.11.attention.self.key.weight ==> roberta.encoder.layers.11.self_attn.k_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.11.attention.self.key.bias ==> roberta.encoder.layers.11.self_attn.k_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.11.attention.self.value.weight ==> roberta.encoder.layers.11.self_attn.v_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.11.attention.self.value.bias ==> roberta.encoder.layers.11.self_attn.v_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.11.attention.output.dense.weight ==> roberta.encoder.layers.11.self_attn.out_proj.weight (torch.Size([768, 768]), True)
roberta.encoder.layer.11.attention.output.dense.bias ==> roberta.encoder.layers.11.self_attn.out_proj.bias (torch.Size([768]), False)
roberta.encoder.layer.11.attention.output.LayerNorm.weight ==> roberta.encoder.layers.11.norm1.weight (torch.Size([768]), False)
roberta.encoder.layer.11.attention.output.LayerNorm.bias ==> roberta.encoder.layers.11.norm1.bias (torch.Size([768]), False)
roberta.encoder.layer.11.intermediate.dense.weight ==> roberta.encoder.layers.11.linear1.weight (torch.Size([768, 3072]), True)